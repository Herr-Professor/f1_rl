model = "Qwen/Qwen3-30B-A3B-Instruct-2507"
max_steps = 200
batch_size = 128
rollouts_per_example = 8
trajectory_strategy = "interleaved"

[sampling]
max_tokens = 700

[[env]]
id = "herr-professor/f1-strategy"
args = { deep_reasoning = true, use_tools = true, multi_turn = false, multi_env = true }

# NOTE: We intentionally omit the [eval] block for reliability.
# Some models can take a long time on the blocking "eval at step 0" phase.
# Run evals separately with:
#   prime eval run herr-professor/f1-strategy -m <model> -n 50 -r 2 --save-results --skip-upload -a '<env_args_json>'

